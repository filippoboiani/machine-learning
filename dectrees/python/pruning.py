import monkdata as m
import dtree as dtree
import drawtree_qt5 as drawLib
import random
import numpy
import matplotlib.pyplot as plt

# creates 2 partition of a dataset with random shuffling
def partition(data, fraction):
    ldata = list(data)
    random.shuffle(ldata)
    breakPoint = int(len(ldata) * fraction)
    return ldata[:breakPoint], ldata[breakPoint:]

# pruning
def myPrune(tree, validation):
    # the tree is the current candidate
    candidate = tree
    
    # compute all the possible prunes for a tree
    newPrunes = dtree.allPruned(candidate)
    for prunedTree in newPrunes:
        if(dtree.check(candidate, validation) < dtree.check(prunedTree, validation)):
            # This pruned tree is better then candidate, thus prune this pruned tree
            # to ensure it is best performing option
            candidate = myPrune(prunedTree, validation)
        else:
            # if the current candidate performs better then one of prunedTree,
            # continue checking other prunes to ensure this is best candidate
            continue

    return candidate


datasets = [{
    'name': 'monk1',
    'ref' : m.monk1,
    'test': m.monk1test
},
{
    'name': 'monk3',
    'ref' : m.monk3,
    'test': m.monk3test
}]

# Define fractions to be used
fractions = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]

avaraged_values = 100

# Prepare result structure, filliwng it with 0s - which will be
# filled with results of pruning tests in the next steps
results = {'monk1': [[0 for x in range(avaraged_values)] for y in range(6)] ,
           'monk3': [[0 for x in range(avaraged_values)] for y in range(6)]}

# test again monk1 and monk2 datasets
for dataset in datasets:
    #identify datasets by variable index
    index = 0
    print "\n\nDataset:     ", dataset['name']

    # Test dataset for each fraction
    for f in fractions: 
        print "\n\nFraction:    ", f

        # Repeat many times to reduce randomnes generated by partitioning of dataset (which is random)
        for i in range(avaraged_values):
            # split the reference dataset into training and validation set.
            monktrain, monkval = partition(dataset['ref'], f)

            # build the dtree from the training set
            decisionTree = dtree.buildTree(monktrain, m.attributes)

            # Performs the complete pruning by repeatedly calling
            # allPruned and picking the tree which gives the best classification perfor-
            # mance on the validation dataset. Pruning is stopped when all the
            # pruned trees perform worse than the current candidate.
            candidate = myPrune(decisionTree, monkval)

            print "Original/Optimized performance with validation: %s/%s"\
                  %(dtree.check(decisionTree, monkval), dtree.check(candidate, monkval))
            #drawLib.draw(candidate)
            #drawLib.draw(decisionTree)

            # Check pruned best performing candidate with test dataset and add to results set
            results[dataset['name']][index][i] = dtree.check(candidate, dataset['test'])
        
        index += 1    

print "\nResults:"

averages = {
    'monk1': [],
    'monk3': []
}

variances = {
    'monk1': [],
    'monk3': []
}

for x in results['monk1']:
    averages['monk1'].append(1- numpy.mean(x))
    variances['monk1'].append(numpy.var(x))

for x in results['monk3']:
    averages['monk3'].append(1- numpy.mean(x))
    variances['monk3'].append(numpy.var(x))

print "Mean:        ", averages
print "Variances:   ", variances


fig, axarr = plt.subplots(2)
ax1 = axarr[0]
ax1.plot(fractions, averages['monk1'], 'b-', label="MONK-1-AVG")
ax1.plot(fractions, averages['monk3'], 'r-', label="MONK-3-AVG")
ax1.set_ylabel('Classification Error \n(against test set)')
ax1.set_xlabel('Train/Validation partition fraction')
ax1.legend(loc='lower right')

ax2 = axarr[1]
ax2.plot(fractions, variances['monk1'], 'b--', label="MONK-1-VAR")
ax2.plot(fractions, variances['monk3'], 'r--', label="MONK-3-VAR")
ax2.set_ylabel('Error Variation \n(against test set)')
fig.tight_layout()
ax2.legend(loc='lower right')
plt.show()
# for i in range(0,1000000):
#     monk1train, monk1val = partition(m.monk1, 0.6)
#     # Rebuild the tree
#     decisionTree = dtree.buildTree(monk1train, m.attributes)
#     prunedTrees = dtree.allPruned(decisionTree)
#     for local in prunedTrees:
#         print local
#         print  "\n"
#         print "check = ", dtree.check(local, monk1train)
#         print "check = ", dtree.check(local, monk1val)
#         if dtree.check(local, monk1val) == 1.0:
#             print local
#             print "Check test: ", dtree.check(local, m.monk1test) # 0.972222222222
#             drawLib.drawTree(local)
#             #for sample in monk1train:
#             #    print "class = ", dtree.classify(local, sample)


# for x in range(0,6) {
#     pruned =  dtree.allPruned(decisionTree)
    
#     print ""
#     print dtree.classify(pruned[0], monk1val)

# }
    


